{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1436971c-e467-4eb7-899f-05507f83400a",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "!pip install  -U git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils wandb accelerate\n",
    "# Tested with transformers==4.47.0.dev0, trl==0.12.0.dev0, datasets==3.0.2, bitsandbytes==0.44.1, peft==0.13.2, qwen-vl-utils==0.0.8, wandb==0.18.5, accelerate==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9226e593-ffc0-43de-9267-4e7dea5bc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('NER_annotations_with_texts_2_TestTrainVal.json', 'r', encoding='utf-8') as fr:\n",
    "    data = json.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05025f6c-d238-405a-98dc-f0d12d85c949",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "unique_ner = list()\n",
    "for i in range(len(data['annotations'])):\n",
    "    annotation = data['annotations'][i]\n",
    "    attributes = annotation.get(\"attributes\", {})\n",
    "    \n",
    "    if \"TestTrainVal\" not in attributes:\n",
    "        continue\n",
    "    labels = [ent[\"entity\"] for ent in attributes.get(\"ner_entities\", [])]\n",
    "    unique_ner.extend(labels)\n",
    "print(set(unique_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98ba7604-f185-4fe5-8a53-20ee2c3bf470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "def get_image(image_path, bbox):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert BGR (OpenCV default) to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert bounding box coordinates to integers\n",
    "    x, y, width, height = map(int, bbox)\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_image = image[y:y+height, x:x+width]\n",
    "\n",
    "    # Convert cropped image to PIL Image\n",
    "    pil_image = Image.fromarray(cropped_image)\n",
    "\n",
    "    # Save to a BytesIO buffer as PNG and reload as PngImageFile\n",
    "    buffer = BytesIO()\n",
    "    pil_image.save(buffer, format=\"PNG\")\n",
    "    buffer.seek(0)\n",
    "    png_image = Image.open(buffer)\n",
    "\n",
    "    return png_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d2405-deea-4cc6-bbbb-095c9f3d7006",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pathIMG = \"path_to_images\"\n",
    "# Initialize dataset structure\n",
    "dataset = {\"image\": [], \"label\": [], \"split\": []}\n",
    "for annotation in tqdm(data[\"annotations\"], desc='Preparing Dataset'):\n",
    "    attributes = annotation.get(\"attributes\", {})\n",
    "    \n",
    "    if \"TestTrainVal\" not in attributes:\n",
    "        continue\n",
    "\n",
    "    idx_actual_image = annotation[\"image_id\"] - 1\n",
    "    image_path = pathIMG + data[\"images\"][idx_actual_image][\"file_name\"]\n",
    "    bbox = annotation[\"bbox\"]\n",
    "    cropped_image = get_image(image_path, bbox)\n",
    "\n",
    "    labels = {\n",
    "                'qwen_ocr_text': attributes.get(\"Text\", '')\n",
    "             }\n",
    "\n",
    "    dataset_key = attributes[\"TestTrainVal\"]\n",
    "    dataset[\"image\"].append(cropped_image)\n",
    "    dataset[\"label\"].append(labels)\n",
    "    dataset[\"split\"].append(attributes[\"TestTrainVal\"])  # Stores \"train\", \"test\", or \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f79189-66f6-40b8-928c-48fa48bc017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Convert dictionary to Hugging Face dataset\n",
    "hf_dataset = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91ba9655-5581-46c6-abf1-3f799126316d",
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "# Split dataset based on the \"split\" column\n",
    "train_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "test_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "val_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "# Remove the \"split\" column as it's no longer needed\n",
    "train_dataset = train_dataset.remove_columns(\"split\")\n",
    "test_dataset = test_dataset.remove_columns(\"split\")\n",
    "val_dataset = val_dataset.remove_columns(\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148a4ac4-c808-4515-8b7e-58b0e246b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "for i in range(0, len(hf_dataset) - 1):\n",
    "    sizes.append(hf_dataset[i]['image'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3fb0beb-9f21-4fed-a013-63a193086fc7",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Extract widths and heights\n",
    "widths = np.array([size[0] for size in sizes])\n",
    "heights = np.array([size[1] for size in sizes])\n",
    "\n",
    "### 1. DETECT OUTLIERS\n",
    "\n",
    "# PERCENTILE METHOD\n",
    "percentile_threshold = 99.5  # Remove top 1% largest images\n",
    "w_percentile = np.percentile(widths, percentile_threshold)\n",
    "h_percentile = np.percentile(heights, percentile_threshold)\n",
    "percentile_outliers = {(w, h) for w, h in zip(widths, heights) if w > w_percentile or h > h_percentile}\n",
    "\n",
    "# Combine all outliers\n",
    "all_outliers = percentile_outliers\n",
    "\n",
    "### 2. REMOVE OUTLIERS FROM hf_dataset\n",
    "filtered_dataset = [hf_dataset[i] for i in range(len(hf_dataset)) if hf_dataset[i]['image'].size not in all_outliers]\n",
    "\n",
    "### 3. PRINT RESULTS\n",
    "print(f\"Original dataset size: {len(hf_dataset)}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "print(f\"Removed {len(hf_dataset) - len(filtered_dataset)} outliers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc0a422-3cda-476d-8a6a-4d4aa22e2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to Hugging Face Dataset\n",
    "tmp_dataset = {}\n",
    "tmp_dataset['image'] = [rec['image'] for rec in filtered_dataset]\n",
    "tmp_dataset['label'] = [rec['label'] for rec in filtered_dataset]\n",
    "tmp_dataset['split'] = [rec['split'] for rec in filtered_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cfc0856-3ede-43c7-86ee-fcb53ed7e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Convert dictionary to Hugging Face dataset\n",
    "filtered_dataset = Dataset.from_dict(tmp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "542812ea-a541-4200-b1ed-fd46c80074c7",
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "# Split dataset based on the \"split\" column\n",
    "train_dataset = filtered_dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "test_dataset = filtered_dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "val_dataset = filtered_dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "# Remove the \"split\" column as it's no longer needed\n",
    "train_dataset = train_dataset.remove_columns(\"split\")\n",
    "test_dataset = test_dataset.remove_columns(\"split\")\n",
    "val_dataset = val_dataset.remove_columns(\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0253d570-7fe8-4e88-a209-7f322c355db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a Vision-Language Model specialized in OCR for multilingual images, primarily containing typewritten text in Ukrainian, Russian, and other languages.  \n",
    "\n",
    "Your task is to:  \n",
    "1. **Extract all text** from the provided image with high accuracy.  \n",
    "\n",
    "Respond concisely in a structured format, providing:  \n",
    "- The full extracted text  \n",
    "\n",
    "Focus on precision, ensuring the output reflects the text in the image. Avoid unnecessary explanations or interpretations beyond the detected content.\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"Perform OCR on the provided image containing typewritten multilingual text (primarily Ukrainian, Russian, and other languages).  \n",
    "Extract and transcribe all text from the entire scanned document accurately.  \n",
    "\n",
    "Return the output in a structured JSON format with the following key:  \n",
    "- `\"qwen_ocr_text\"`: The complete transcribed text from the document  \n",
    "\n",
    "Ensure high accuracy in text extraction. Do not include unnecessary explanations in the response.\n",
    "\"\"\"  \n",
    "\n",
    "def format_data(sample):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": sample[\"image\"],\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": json.dumps(sample['label'], indent=2, ensure_ascii=False)}],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40f1e9da-b60c-4236-87c1-5fd2d1b11f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [format_data(sample) for sample in train_dataset]\n",
    "val_dataset = [format_data(sample) for sample in val_dataset]\n",
    "test_dataset = [format_data(sample) for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edb64952-9e5c-44db-9d7b-1cc9aad5ba18",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    model = None\n",
    "    processor = None\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a450a1b-9592-40f7-9f09-5ac4050fb305",
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=bnb_config\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b247da4-5e28-4001-8212-2fe73e9eb29f",
   "metadata": {},
   "outputs": [
   
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd48cd71-8b22-4a98-8e75-2d8c28c8b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cf5b1-566c-4324-bf65-039e38fef858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"output_dir\",  # Directory to save the model\n",
    "    num_train_epochs=8,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size for training\n",
    "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=8,  # Steps to accumulate gradients\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    # Optimizer and scheduler settings\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
    "    learning_rate=2e-4,  # Learning rate for training\n",
    "    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,  # Steps interval for logging\n",
    "    eval_steps=100,  # Steps interval for evaluation\n",
    "    eval_strategy=\"steps\",  # Strategy for evaluation\n",
    "    save_strategy=\"steps\",  # Strategy for saving the model\n",
    "    save_steps=100,  # Steps interval for saving\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n",
    "    greater_is_better=False,  # Whether higher metric values are better\n",
    "    load_best_model_at_end=True,  # Load the best model after training\n",
    "    # Mixed precision and gradient settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    tf32=True,  # Use TensorFloat-32 precision\n",
    "    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
    "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
    "    # Hub and reporting\n",
    "    push_to_hub=True,  # Whether to push model to Hugging Face Hub\n",
    "    report_to=\"none\",  # Reporting tool for tracking metrics\n",
    "    # Gradient checkpointing settings\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
    "    # Dataset configuration\n",
    "    dataset_text_field=\"\",  # Text field in dataset\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n",
    "    # max_seq_length=1024  # Maximum sequence length for input\n",
    ")\n",
    "\n",
    "training_args.remove_unused_columns = False  # Keep unused columns in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f80eece-11a7-484f-8dd8-b996584c886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen2_5_VLProcessor\n",
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    # Get the texts and images, and apply the chat template\n",
    "    texts = [\n",
    "        processor.apply_chat_template(example, tokenize=False) for example in examples\n",
    "    ]  # Prepare texts for processing\n",
    "    image_inputs = [process_vision_info(example)[0] for example in examples]  # Process the images to extract inputs\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(\n",
    "        text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n",
    "    )  # Encode texts and images into tensors\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()  # Clone input IDs for labels\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n",
    "\n",
    "    # Ignore the image token index in the loss computation (model specific)\n",
    "    if isinstance(processor, Qwen2_5_VLProcessor):  # Check if the processor is Qwen2VLProcessor\n",
    "        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n",
    "    else:\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n",
    "\n",
    "    # Mask image token IDs in the labels\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
    "\n",
    "    batch[\"labels\"] = labels  # Add labels to the batch\n",
    "\n",
    "    return batch  # Return the prepared batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbd2f084-6991-4e1b-81e5-cfad965669b8",
   "metadata": {},
   "outputs": [

   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fc18e60-cee0-46b0-a2a7-fccf9a357dcf",
   "metadata": {},
   "outputs": [
    
    
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a6a80c0-6598-4c3b-ba94-e3de36baa2bb",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "trainer.save_model(training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
