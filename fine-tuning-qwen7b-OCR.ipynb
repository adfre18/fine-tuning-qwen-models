{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1436971c-e467-4eb7-899f-05507f83400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "Collecting git+https://github.com/huggingface/transformers.git\n",
      "  Cloning https://github.com/huggingface/transformers.git to /scratch.shared/1/afremund/job_10665424.pbs-m1/pip-req-build-_qalfyub\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /scratch.shared/1/afremund/job_10665424.pbs-m1/pip-req-build-_qalfyub\n",
      "  Resolved https://github.com/huggingface/transformers.git to commit a847d4aa6bd2279f5be235dc0fd862f58f7403d1\n",
      "  Installing build dependencies ... done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25done\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?2done\n",
      "\u001b[?25hCollecting git+https://github.com/huggingface/trl.git\n",
      "  Cloning https://github.com/huggingface/trl.git to /scratch.shared/1/afremund/job_10665424.pbs-m1/pip-req-build-hddfk48e\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/trl.git /scratch.shared/1/afremund/job_10665424.pbs-m1/pip-req-build-hddfk48e\n",
      "  Resolved https://github.com/huggingface/trl.git to commit 2bf48478e888b9dcbb47c3e577e9f0f004d17c35\n",
      "  Installing build dependencies ... \u001b[done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: datasets in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (3.5.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: bitsandbytes in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (0.45.5)\n",
      "Requirement already satisfied: peft in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (0.15.2)\n",
      "Requirement already satisfied: qwen-vl-utils in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (0.0.11)\n",
      "Requirement already satisfied: wandb in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (0.19.10)\n",
      "Requirement already satisfied: accelerate in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.0.dev0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from transformers==4.52.0.dev0) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.0.dev0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.0.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.0.dev0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.0.dev0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.0.dev0) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.0.dev0) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.52.0.dev0) (4.67.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from trl==0.18.0.dev0) (13.9.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets) (3.11.9)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from bitsandbytes) (2.6.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (6.1.0)\n",
      "Requirement already satisfied: av in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from qwen-vl-utils) (14.2.0)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from qwen-vl-utils) (11.0.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.21.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.12/dist-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from wandb) (70.3.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.0.dev0) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.0.dev0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.0.dev0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.52.0.dev0) (2024.8.30)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.18.0.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->trl==0.18.0.dev0) (2.18.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.18.0.dev0) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "Installing collected packages: datasets\n",
      "  Attempting uninstall: datasets\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution ~atasets (/storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: datasets 3.5.0\n",
      "    Uninstalling datasets-3.5.0:\n",
      "      Successfully uninstalled datasets-3.5.0\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "Successfully installed datasets-3.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install  -U git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils wandb accelerate\n",
    "# Tested with transformers==4.47.0.dev0, trl==0.12.0.dev0, datasets==3.0.2, bitsandbytes==0.44.1, peft==0.13.2, qwen-vl-utils==0.0.8, wandb==0.18.5, accelerate==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9226e593-ffc0-43de-9267-4e7dea5bc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('NER_annotations_with_texts_2_TestTrainVal.json', 'r', encoding='utf-8') as fr:\n",
    "    data = json.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05025f6c-d238-405a-98dc-f0d12d85c949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ORG', 'LOCATION', 'PERSON', 'DATE'}\n"
     ]
    }
   ],
   "source": [
    "unique_ner = list()\n",
    "for i in range(len(data['annotations'])):\n",
    "    annotation = data['annotations'][i]\n",
    "    attributes = annotation.get(\"attributes\", {})\n",
    "    \n",
    "    if \"TestTrainVal\" not in attributes:\n",
    "        continue\n",
    "    labels = [ent[\"entity\"] for ent in attributes.get(\"ner_entities\", [])]\n",
    "    unique_ner.extend(labels)\n",
    "print(set(unique_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98ba7604-f185-4fe5-8a53-20ee2c3bf470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "def get_image(image_path, bbox):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert BGR (OpenCV default) to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert bounding box coordinates to integers\n",
    "    x, y, width, height = map(int, bbox)\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_image = image[y:y+height, x:x+width]\n",
    "\n",
    "    # Convert cropped image to PIL Image\n",
    "    pil_image = Image.fromarray(cropped_image)\n",
    "\n",
    "    # Save to a BytesIO buffer as PNG and reload as PngImageFile\n",
    "    buffer = BytesIO()\n",
    "    pil_image.save(buffer, format=\"PNG\")\n",
    "    buffer.seek(0)\n",
    "    png_image = Image.open(buffer)\n",
    "\n",
    "    return png_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d2405-deea-4cc6-bbbb-095c9f3d7006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Dataset: 100%|██████████| 89788/89788 [22:56<00:00, 65.24it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pathIMG = \"path_to_images\"\n",
    "# Initialize dataset structure\n",
    "dataset = {\"image\": [], \"label\": [], \"split\": []}\n",
    "for annotation in tqdm(data[\"annotations\"], desc='Preparing Dataset'):\n",
    "    attributes = annotation.get(\"attributes\", {})\n",
    "    \n",
    "    if \"TestTrainVal\" not in attributes:\n",
    "        continue\n",
    "\n",
    "    idx_actual_image = annotation[\"image_id\"] - 1\n",
    "    image_path = pathIMG + data[\"images\"][idx_actual_image][\"file_name\"]\n",
    "    bbox = annotation[\"bbox\"]\n",
    "    cropped_image = get_image(image_path, bbox)\n",
    "\n",
    "    labels = {\n",
    "                'qwen_ocr_text': attributes.get(\"Text\", '')\n",
    "             }\n",
    "\n",
    "    dataset_key = attributes[\"TestTrainVal\"]\n",
    "    dataset[\"image\"].append(cropped_image)\n",
    "    dataset[\"label\"].append(labels)\n",
    "    dataset[\"split\"].append(attributes[\"TestTrainVal\"])  # Stores \"train\", \"test\", or \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36f79189-66f6-40b8-928c-48fa48bc017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Convert dictionary to Hugging Face dataset\n",
    "hf_dataset = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91ba9655-5581-46c6-abf1-3f799126316d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109522521a5b49408460c01f744706b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8016676a12a7406c86530b97943f3bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa11ca49abf4443c9038ea56a04ca28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split dataset based on the \"split\" column\n",
    "train_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "test_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "val_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "# Remove the \"split\" column as it's no longer needed\n",
    "train_dataset = train_dataset.remove_columns(\"split\")\n",
    "test_dataset = test_dataset.remove_columns(\"split\")\n",
    "val_dataset = val_dataset.remove_columns(\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148a4ac4-c808-4515-8b7e-58b0e246b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "for i in range(0, len(hf_dataset) - 1):\n",
    "    sizes.append(hf_dataset[i]['image'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3fb0beb-9f21-4fed-a013-63a193086fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 11030\n",
      "Filtered dataset size: 10921\n",
      "Removed 109 outliers.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Extract widths and heights\n",
    "widths = np.array([size[0] for size in sizes])\n",
    "heights = np.array([size[1] for size in sizes])\n",
    "\n",
    "### 1. DETECT OUTLIERS\n",
    "\n",
    "# PERCENTILE METHOD\n",
    "percentile_threshold = 99.5  # Remove top 1% largest images\n",
    "w_percentile = np.percentile(widths, percentile_threshold)\n",
    "h_percentile = np.percentile(heights, percentile_threshold)\n",
    "percentile_outliers = {(w, h) for w, h in zip(widths, heights) if w > w_percentile or h > h_percentile}\n",
    "\n",
    "# Combine all outliers\n",
    "all_outliers = percentile_outliers\n",
    "\n",
    "### 2. REMOVE OUTLIERS FROM hf_dataset\n",
    "filtered_dataset = [hf_dataset[i] for i in range(len(hf_dataset)) if hf_dataset[i]['image'].size not in all_outliers]\n",
    "\n",
    "### 3. PRINT RESULTS\n",
    "print(f\"Original dataset size: {len(hf_dataset)}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "print(f\"Removed {len(hf_dataset) - len(filtered_dataset)} outliers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bc0a422-3cda-476d-8a6a-4d4aa22e2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to Hugging Face Dataset\n",
    "tmp_dataset = {}\n",
    "tmp_dataset['image'] = [rec['image'] for rec in filtered_dataset]\n",
    "tmp_dataset['label'] = [rec['label'] for rec in filtered_dataset]\n",
    "tmp_dataset['split'] = [rec['split'] for rec in filtered_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cfc0856-3ede-43c7-86ee-fcb53ed7e68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Convert dictionary to Hugging Face dataset\n",
    "filtered_dataset = Dataset.from_dict(tmp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "542812ea-a541-4200-b1ed-fd46c80074c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ee05e76db84c5c850c67f617d37fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10921 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b335ad10ff4801a7cf021dfeda59c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10921 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bba150469b24c83ba0034ae3b95f68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10921 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split dataset based on the \"split\" column\n",
    "train_dataset = filtered_dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "test_dataset = filtered_dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "val_dataset = filtered_dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "# Remove the \"split\" column as it's no longer needed\n",
    "train_dataset = train_dataset.remove_columns(\"split\")\n",
    "test_dataset = test_dataset.remove_columns(\"split\")\n",
    "val_dataset = val_dataset.remove_columns(\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0253d570-7fe8-4e88-a209-7f322c355db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a Vision-Language Model specialized in OCR for multilingual images, primarily containing typewritten text in Ukrainian, Russian, and other languages.  \n",
    "\n",
    "Your task is to:  \n",
    "1. **Extract all text** from the provided image with high accuracy.  \n",
    "\n",
    "Respond concisely in a structured format, providing:  \n",
    "- The full extracted text  \n",
    "\n",
    "Focus on precision, ensuring the output reflects the text in the image. Avoid unnecessary explanations or interpretations beyond the detected content.\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"Perform OCR on the provided image containing typewritten multilingual text (primarily Ukrainian, Russian, and other languages).  \n",
    "Extract and transcribe all text from the entire scanned document accurately.  \n",
    "\n",
    "Return the output in a structured JSON format with the following key:  \n",
    "- `\"qwen_ocr_text\"`: The complete transcribed text from the document  \n",
    "\n",
    "Ensure high accuracy in text extraction. Do not include unnecessary explanations in the response.\n",
    "\"\"\"  \n",
    "\n",
    "def format_data(sample):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": sample[\"image\"],\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": json.dumps(sample['label'], indent=2, ensure_ascii=False)}],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40f1e9da-b60c-4236-87c1-5fd2d1b11f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [format_data(sample) for sample in train_dataset]\n",
    "val_dataset = [format_data(sample) for sample in val_dataset]\n",
    "test_dataset = [format_data(sample) for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edb64952-9e5c-44db-9d7b-1cc9aad5ba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU allocated memory: 0.00 GB\n",
      "GPU reserved memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    model = None\n",
    "    processor = None\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a450a1b-9592-40f7-9f09-5ac4050fb305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 11:37:44.828399: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-29 11:37:45.449059: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-29 11:37:45.652243: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8473] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-29 11:37:45.705907: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1471] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-29 11:37:46.138084: I tensorflow/core/platform/cpu_feature_guard.cc:211] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652b87d428af4633bd1b7b206d182a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=bnb_config\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b247da4-5e28-4001-8212-2fe73e9eb29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,523,136 || all params: 8,294,689,792 || trainable%: 0.0304\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd48cd71-8b22-4a98-8e75-2d8c28c8b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cf5b1-566c-4324-bf65-039e38fef858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"output_dir\",  # Directory to save the model\n",
    "    num_train_epochs=8,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size for training\n",
    "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=8,  # Steps to accumulate gradients\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    # Optimizer and scheduler settings\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
    "    learning_rate=2e-4,  # Learning rate for training\n",
    "    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,  # Steps interval for logging\n",
    "    eval_steps=100,  # Steps interval for evaluation\n",
    "    eval_strategy=\"steps\",  # Strategy for evaluation\n",
    "    save_strategy=\"steps\",  # Strategy for saving the model\n",
    "    save_steps=100,  # Steps interval for saving\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n",
    "    greater_is_better=False,  # Whether higher metric values are better\n",
    "    load_best_model_at_end=True,  # Load the best model after training\n",
    "    # Mixed precision and gradient settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    tf32=True,  # Use TensorFloat-32 precision\n",
    "    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
    "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
    "    # Hub and reporting\n",
    "    push_to_hub=True,  # Whether to push model to Hugging Face Hub\n",
    "    report_to=\"none\",  # Reporting tool for tracking metrics\n",
    "    # Gradient checkpointing settings\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
    "    # Dataset configuration\n",
    "    dataset_text_field=\"\",  # Text field in dataset\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n",
    "    # max_seq_length=1024  # Maximum sequence length for input\n",
    ")\n",
    "\n",
    "training_args.remove_unused_columns = False  # Keep unused columns in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f80eece-11a7-484f-8dd8-b996584c886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen2_5_VLProcessor\n",
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    # Get the texts and images, and apply the chat template\n",
    "    texts = [\n",
    "        processor.apply_chat_template(example, tokenize=False) for example in examples\n",
    "    ]  # Prepare texts for processing\n",
    "    image_inputs = [process_vision_info(example)[0] for example in examples]  # Process the images to extract inputs\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(\n",
    "        text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n",
    "    )  # Encode texts and images into tensors\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()  # Clone input IDs for labels\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n",
    "\n",
    "    # Ignore the image token index in the loss computation (model specific)\n",
    "    if isinstance(processor, Qwen2_5_VLProcessor):  # Check if the processor is Qwen2VLProcessor\n",
    "        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n",
    "    else:\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n",
    "\n",
    "    # Mask image token IDs in the labels\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
    "\n",
    "    batch[\"labels\"] = labels  # Add labels to the batch\n",
    "\n",
    "    return batch  # Return the prepared batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbd2f084-6991-4e1b-81e5-cfad965669b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fc18e60-cee0-46b0-a2a7-fccf9a357dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2176' max='2176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2176/2176 5:08:06, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.052400</td>\n",
       "      <td>0.057661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.050800</td>\n",
       "      <td>0.053958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.051726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.050835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.043300</td>\n",
       "      <td>0.049292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.048990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.048457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.049000</td>\n",
       "      <td>0.047956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.037700</td>\n",
       "      <td>0.047802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>0.048214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.047603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>0.048280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.048022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.049431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.034900</td>\n",
       "      <td>0.048987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.048688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.050566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.033700</td>\n",
       "      <td>0.050148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.039500</td>\n",
       "      <td>0.050172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.052790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.030600</td>\n",
       "      <td>0.051784</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2176, training_loss=0.08643319615272477, metrics={'train_runtime': 18515.2975, 'train_samples_per_second': 3.76, 'train_steps_per_second': 0.118, 'total_flos': 2.5206661176820224e+18, 'train_loss': 0.08643319615272477})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a6a80c0-6598-4c3b-ba94-e3de36baa2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
