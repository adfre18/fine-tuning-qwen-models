{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dbe5598-c617-4211-9f05-74ab7b524c9b",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302250f1-04e1-4ac6-be39-85bde408a43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d25a6-3c9e-49f7-8dad-7ae4c3376e28",
   "metadata": {},
   "source": [
    "### Loading model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d695b2c-4382-494e-a303-dcca4242543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\" # can be changed to 7B model\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55287660-a331-4f6d-a922-8c568e926979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using adapter\n",
    "adapter_path = \"adapter_path\"\n",
    "model.load_adapter(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641dbba-4308-4606-aabe-816bd3b9c27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# dataset loading - this dataset was created during the fine-tuning process\n",
    "hf_dataset = load_from_disk('dataset_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07fcead2-dcae-4975-9d6d-6bde404a0645",
   "metadata": {},
   "outputs": [
    
    
    
   ],
   "source": [
    "# Split dataset based on the \"split\" column\n",
    "train_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "test_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "val_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "# Remove the \"split\" column as it's no longer needed\n",
    "train_dataset = train_dataset.remove_columns(\"split\")\n",
    "test_dataset = test_dataset.remove_columns(\"split\")\n",
    "val_dataset = val_dataset.remove_columns(\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d734c7e-9e49-4079-a477-839152f37501",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a Vision Language Model specialized in detecting named entities in Ukrainian images containing typewritten words or sentences.\n",
    "Your task is to analyze the provided image and identify named entities such as names, locations, organizations, and other key terms.\n",
    "Respond concisely, typically providing the detected entities as a structured list or short phrases.\n",
    "Focus on accuracy and ensure the extracted entities reflect the text in the image. Avoid additional explanations unless absolutely necessary.\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"Analyze the provided image containing typewritten Ukrainian text. \n",
    "Detect and extract named entities belonging to the following categories:\n",
    "- PERSON (names of individuals)\n",
    "- LOCATION (geographical places, cities, countries)\n",
    "- DATE (specific dates, years, or time-related expressions)\n",
    "- ORG (organizations, institutions, or companies)\n",
    "\n",
    "Respond with a structured list of detected entities along with their corresponding entity types. \n",
    "Ensure accuracy and avoid adding unnecessary explanations.\"\"\"\n",
    "\n",
    "def format_data(sample):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": sample[\"image\"],\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": json.dumps(sample['label'], indent=2, ensure_ascii=False)}],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7a1f519-6e93-4746-b790-e1f14ceeb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [format_data(sample) for sample in train_dataset]\n",
    "val_dataset = [format_data(sample) for sample in val_dataset]\n",
    "test_dataset = [format_data(sample) for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1a75546-1a47-4d92-8293-7d1676218f75",
   "metadata": {},
   "outputs": [

   ],
   "source": [
    "# this will be sent to model\n",
    "train_dataset[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "191ba5a0-22da-4837-ade8-1251996d2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate function - prepares all necessary data for the inference\n",
    "def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=\"cuda\"):\n",
    "    # Prepare the text input by applying the chat template\n",
    "    text_input = processor.apply_chat_template(\n",
    "        sample[1:2], tokenize=False, add_generation_prompt=True  # Use the sample without the system message\n",
    "    )\n",
    "\n",
    "    # Process the visual input from the sample\n",
    "    image_inputs, _ = process_vision_info(sample)\n",
    "\n",
    "    # Prepare the inputs for the model\n",
    "    model_inputs = processor(\n",
    "        text=[text_input],\n",
    "        images=image_inputs,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\n",
    "        device\n",
    "    )  # Move inputs to the specified device\n",
    "\n",
    "    # Generate text with the model\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens, temperature=0.1)\n",
    "\n",
    "    # Trim the generated ids to remove the input ids\n",
    "    trimmed_generated_ids = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "    # Decode the output text\n",
    "    output_text = processor.batch_decode(\n",
    "        trimmed_generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    return output_text[0]  # Return the first decoded output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4ee01900-5b04-4186-9da3-62fded36d8e5",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "for i in range(0, len(test_dataset)-1):\n",
    "    output = generate_text_from_sample(model, processor, test_dataset[i])\n",
    "    if \"entity\" in str(test_dataset[i][2]):\n",
    "        print('Model Prediction: ', eval(output))\n",
    "        print('Truth Data: ', eval(test_dataset[i][2]['content'][0]['text']))\n",
    "        display(test_dataset[i][1]['content'][0]['image'])\n",
    "        print('-'*100)\n",
    "        break\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
