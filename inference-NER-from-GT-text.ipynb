{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dbe5598-c617-4211-9f05-74ab7b524c9b",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302250f1-04e1-4ac6-be39-85bde408a43e",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1d25a6-3c9e-49f7-8dad-7ae4c3376e28",
   "metadata": {},
   "source": [
    "### Loading model and processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d695b2c-4382-494e-a303-dcca4242543e",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55287660-a331-4f6d-a922-8c568e926979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using adapter\n",
    "adapter_path = \"adapter_path\"\n",
    "model.load_adapter(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96874e0c-0739-4504-85e0-eb4a0c64ee28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\n",
    "    \"NER_annotations_with_texts_2_TestTrainVal_Qwen7BftOCR.json\", \"r\", encoding=\"utf-8\"\n",
    ") as fr:\n",
    "    data = json.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb5a34ae-7faa-4905-91c7-6a633e925782",
   "metadata": {},
   "outputs": [
   
   ],
   "source": [
    "unique_ner = list()\n",
    "for i in range(len(data[\"annotations\"])):\n",
    "    annotation = data[\"annotations\"][i]\n",
    "    attributes = annotation.get(\"attributes\", {})\n",
    "\n",
    "    if \"TestTrainVal\" not in attributes:\n",
    "        continue\n",
    "    labels = [ent[\"entity\"] for ent in attributes.get(\"ner_entities\", [])]\n",
    "    unique_ner.extend(labels)\n",
    "print(set(unique_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a346137-d734-4fcb-a70d-9c7dd472b118",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize dataset structure\n",
    "dataset = {\"input_text\": [], \"label\": [], \"split\": []}\n",
    "for annotation in tqdm(data[\"annotations\"], desc=\"Preparing Dataset\"):\n",
    "    attributes = annotation.get(\"attributes\", {})\n",
    "    if \"TestTrainVal\" not in attributes:\n",
    "        continue\n",
    "    labels = [\n",
    "        {\"entity\": ent[\"entity\"], \"text\": ent[\"text\"]}\n",
    "        for ent in attributes.get(\"ner_entities\", [])\n",
    "    ]\n",
    "\n",
    "    dataset_key = attributes[\"TestTrainVal\"]\n",
    "    dataset[\"label\"].append(labels)\n",
    "    dataset[\"input_text\"].append(attributes[\"Text\"]) # this is the ground truth text - can be changed to OCR text\n",
    "    dataset[\"split\"].append(\n",
    "        attributes[\"TestTrainVal\"]\n",
    "    )  # Stores \"train\", \"test\", or \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f544306b-c3d1-40af-bdda-846dd3f1bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert dictionary to Hugging Face dataset\n",
    "hf_dataset = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa573f95-fd96-479a-9e15-f1e28320f19e",
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "# Split dataset based on the \"split\" column\n",
    "train_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "test_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "val_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "# Remove the \"split\" column as it's no longer needed\n",
    "train_dataset = train_dataset.remove_columns(\"split\")\n",
    "test_dataset = test_dataset.remove_columns(\"split\")\n",
    "val_dataset = val_dataset.remove_columns(\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b995964-e900-4470-8c7e-1c0d54744957",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a Language Model specialized in detecting named entities in Ukrainian texts.\n",
    "Your task is to analyze the provided text and identify named entities such as names, locations, organizations, and other key terms.\n",
    "Respond concisely, typically providing the detected entities as a structured list or short phrases.\n",
    "Focus on accuracy and ensure the extracted entities reflect the text. Avoid additional explanations unless absolutely necessary.\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"Analyze the provided Ukrainian text. \n",
    "Detect and extract named entities belonging to the following categories:\n",
    "- PERSON (names of individuals)\n",
    "- LOCATION (geographical places, cities, countries)\n",
    "- DATE (specific dates, years, or time-related expressions)\n",
    "- ORG (organizations, institutions, or companies)\n",
    "\n",
    "Respond with a structured list of detected entities along with their corresponding entity types. \n",
    "Ensure accuracy and avoid adding unnecessary explanations.\"\"\"\n",
    "\n",
    "text_prompt = \"\"\"Text:\\n\"\"\"\n",
    "\n",
    "\n",
    "def format_data(sample):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": text_prompt + sample[\"input_text\"],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": json.dumps(sample[\"label\"], indent=2, ensure_ascii=False),\n",
    "                }\n",
    "            ],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7a1f519-6e93-4746-b790-e1f14ceeb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [format_data(sample) for sample in train_dataset]\n",
    "val_dataset = [format_data(sample) for sample in val_dataset]\n",
    "test_dataset = [format_data(sample) for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1a75546-1a47-4d92-8293-7d1676218f75",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "# this will be sent to model\n",
    "train_dataset[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "191ba5a0-22da-4837-ade8-1251996d2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate function - prepares all necessary data for the inference\n",
    "def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device=\"cuda\"):\n",
    "    # Prepare the text input by applying the chat template\n",
    "    text_input = processor.apply_chat_template(\n",
    "        sample[1:2], tokenize=False, add_generation_prompt=True  # Use the sample without the system message\n",
    "    )\n",
    "\n",
    "    # Process the visual input from the sample\n",
    "    #image_inputs, _ = process_vision_info(sample)\n",
    "\n",
    "    # Prepare the inputs for the model\n",
    "    model_inputs = processor(\n",
    "        text=[text_input],\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\n",
    "        device\n",
    "    )  # Move inputs to the specified device\n",
    "\n",
    "    # Generate text with the model\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens, temperature=0.1)\n",
    "\n",
    "    # Trim the generated ids to remove the input ids\n",
    "    trimmed_generated_ids = [out_ids[len(in_ids) :] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "\n",
    "    # Decode the output text\n",
    "    output_text = processor.batch_decode(\n",
    "        trimmed_generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "\n",
    "    return output_text[0]  # Return the first decoded output text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ee01900-5b04-4186-9da3-62fded36d8e5",
   "metadata": {},
   "outputs": [
    
   ],
   "source": [
    "for i in range(0, len(test_dataset)-1):\n",
    "    output = generate_text_from_sample(model, processor, test_dataset[i])\n",
    "    print('Input text:', test_dataset[i][1]['content'][1]['text'])\n",
    "    print('Model Prediction: ', eval(output))\n",
    "    print('Truth Data: ', eval(test_dataset[i][2]['content'][0]['text']))\n",
    "    #display(test_dataset[i][1]['content'][0]['image'])\n",
    "    print('-'*100)\n",
    "    if i > 50:\n",
    "        break\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
