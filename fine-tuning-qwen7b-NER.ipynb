{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1436971c-e467-4eb7-899f-05507f83400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution ~atasets (/storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install  -U -q git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/trl.git datasets bitsandbytes peft qwen-vl-utils wandb accelerate\n",
    "# Tested with transformers==4.47.0.dev0, trl==0.12.0.dev0, datasets==3.0.2, bitsandbytes==0.44.1, peft==0.13.2, qwen-vl-utils==0.0.8, wandb==0.18.5, accelerate==1.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9226e593-ffc0-43de-9267-4e7dea5bc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('NER_annotations_with_texts_2_TestTrainVal.json', 'r', encoding='utf-8') as fr:\n",
    "    data = json.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05025f6c-d238-405a-98dc-f0d12d85c949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ORG', 'DATE', 'LOCATION', 'PERSON'}\n"
     ]
    }
   ],
   "source": [
    "unique_ner = list()\n",
    "for i in range(len(data['annotations'])):\n",
    "    annotation = data['annotations'][i]\n",
    "    attributes = annotation.get(\"attributes\", {})\n",
    "    \n",
    "    if \"TestTrainVal\" not in attributes:\n",
    "        continue\n",
    "    labels = [ent[\"entity\"] for ent in attributes.get(\"ner_entities\", [])]\n",
    "    unique_ner.extend(labels)\n",
    "print(set(unique_ner))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98ba7604-f185-4fe5-8a53-20ee2c3bf470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "def get_image(image_path, bbox):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert BGR (OpenCV default) to RGB\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Convert bounding box coordinates to integers\n",
    "    x, y, width, height = map(int, bbox)\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_image = image[y:y+height, x:x+width]\n",
    "\n",
    "    # Convert cropped image to PIL Image\n",
    "    pil_image = Image.fromarray(cropped_image)\n",
    "\n",
    "    # Save to a BytesIO buffer as PNG and reload as PngImageFile\n",
    "    buffer = BytesIO()\n",
    "    pil_image.save(buffer, format=\"PNG\")\n",
    "    buffer.seek(0)\n",
    "    png_image = Image.open(buffer)\n",
    "\n",
    "    return png_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302d2405-deea-4cc6-bbbb-095c9f3d7006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing Dataset: 100%|██████████| 89788/89788 [23:26<00:00, 63.82it/s]  \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "pathIMG = \"path_to_images\"\n",
    "# Initialize dataset structure\n",
    "dataset = {\"image\": [], \"label\": [], \"split\": []}\n",
    "for annotation in tqdm(data[\"annotations\"], desc='Preparing Dataset'):\n",
    "    attributes = annotation.get(\"attributes\", {})\n",
    "    \n",
    "    if \"TestTrainVal\" not in attributes:\n",
    "        continue\n",
    "\n",
    "    idx_actual_image = annotation[\"image_id\"] - 1\n",
    "    image_path = pathIMG + data[\"images\"][idx_actual_image][\"file_name\"]\n",
    "    bbox = annotation[\"bbox\"]\n",
    "    cropped_image = get_image(image_path, bbox)\n",
    "\n",
    "    labels = [{\"entity\": ent[\"entity\"], \"text\": ent[\"text\"]} for ent in attributes.get(\"ner_entities\", [])]\n",
    "\n",
    "    dataset_key = attributes[\"TestTrainVal\"]\n",
    "    dataset[\"image\"].append(cropped_image)\n",
    "    dataset[\"label\"].append(labels)\n",
    "    dataset[\"split\"].append(attributes[\"TestTrainVal\"])  # Stores \"train\", \"test\", or \"val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36f79189-66f6-40b8-928c-48fa48bc017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Convert dictionary to Hugging Face dataset\n",
    "hf_dataset = Dataset.from_dict(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91ba9655-5581-46c6-abf1-3f799126316d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c54f976b1149dba5b7a87830433985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260660017c7448ccaf9942c4f63d79f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d5b1dd36b7c45e498926cf19632f4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/11030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split dataset based on the \"split\" column\n",
    "train_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "test_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "val_dataset = hf_dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "# Remove the \"split\" column as it's no longer needed\n",
    "train_dataset = train_dataset.remove_columns(\"split\")\n",
    "test_dataset = test_dataset.remove_columns(\"split\")\n",
    "val_dataset = val_dataset.remove_columns(\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2056a35a-5f44-4d7a-808b-75bafcbe9e00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11030"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcf7674b-8448-4b80-b58d-8604cfdd1b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = []\n",
    "for i in range(0, len(hf_dataset) - 1):\n",
    "    sizes.append(hf_dataset[i]['image'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c384e6e-3e36-4c7e-a3b3-b634f3e30b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 11030\n",
      "Filtered dataset size: 10820\n",
      "Removed 210 outliers.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Extract widths and heights\n",
    "widths = np.array([size[0] for size in sizes])\n",
    "heights = np.array([size[1] for size in sizes])\n",
    "\n",
    "### 1. DETECT OUTLIERS\n",
    "\n",
    "# PERCENTILE METHOD\n",
    "percentile_threshold = 99  # Remove top 1% largest images\n",
    "w_percentile = np.percentile(widths, percentile_threshold)\n",
    "h_percentile = np.percentile(heights, percentile_threshold)\n",
    "percentile_outliers = {(w, h) for w, h in zip(widths, heights) if w > w_percentile or h > h_percentile}\n",
    "\n",
    "# Combine all outliers\n",
    "all_outliers = percentile_outliers\n",
    "\n",
    "### 2. REMOVE OUTLIERS FROM hf_dataset\n",
    "filtered_dataset = [hf_dataset[i] for i in range(len(hf_dataset)) if hf_dataset[i]['image'].size not in all_outliers]\n",
    "\n",
    "### 3. PRINT RESULTS\n",
    "print(f\"Original dataset size: {len(hf_dataset)}\")\n",
    "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "print(f\"Removed {len(hf_dataset) - len(filtered_dataset)} outliers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eae9f099-3efa-4846-b206-6048d1930c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to Hugging Face Dataset\n",
    "tmp_dataset = {}\n",
    "tmp_dataset['image'] = [rec['image'] for rec in filtered_dataset]\n",
    "tmp_dataset['label'] = [rec['label'] for rec in filtered_dataset]\n",
    "tmp_dataset['split'] = [rec['split'] for rec in filtered_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "035d58a0-c479-4509-9e00-00d5532aca68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "# Convert dictionary to Hugging Face dataset\n",
    "filtered_dataset = Dataset.from_dict(tmp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f08efb96-d22f-4810-9d1a-610b63a16074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f9ee2bc43b42758621f754ab429e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10820 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e022398b80e4bc6b5aa3fa4107480d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10820 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5270d99901be4432854c0800b851d431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10820 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split dataset based on the \"split\" column\n",
    "train_dataset = filtered_dataset.filter(lambda x: x[\"split\"] == \"train\")\n",
    "test_dataset = filtered_dataset.filter(lambda x: x[\"split\"] == \"test\")\n",
    "val_dataset = filtered_dataset.filter(lambda x: x[\"split\"] == \"val\")\n",
    "\n",
    "# Remove the \"split\" column as it's no longer needed\n",
    "train_dataset = train_dataset.remove_columns(\"split\")\n",
    "test_dataset = test_dataset.remove_columns(\"split\")\n",
    "val_dataset = val_dataset.remove_columns(\"split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0253d570-7fe8-4e88-a209-7f322c355db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a Vision Language Model specialized in detecting named entities in Ukrainian images containing typewritten words or sentences.\n",
    "Your task is to analyze the provided image and identify named entities such as names, locations, organizations, and other key terms.\n",
    "Respond concisely, typically providing the detected entities as a structured list or short phrases.\n",
    "Focus on accuracy and ensure the extracted entities reflect the text in the image. Avoid additional explanations unless absolutely necessary.\"\"\"\n",
    "\n",
    "user_prompt = \"\"\"Analyze the provided image containing typewritten Ukrainian text. \n",
    "Detect and extract named entities belonging to the following categories:\n",
    "- PERSON (names of individuals)\n",
    "- LOCATION (geographical places, cities, countries)\n",
    "- DATE (specific dates, years, or time-related expressions)\n",
    "- ORG (organizations, institutions, or companies)\n",
    "\n",
    "Respond with a structured list of detected entities along with their corresponding entity types. \n",
    "Ensure accuracy and avoid adding unnecessary explanations.\"\"\"\n",
    "\n",
    "def format_data(sample):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": sample[\"image\"],\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": user_prompt,\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": json.dumps(sample['label'], indent=2, ensure_ascii=False)}],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40f1e9da-b60c-4236-87c1-5fd2d1b11f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = [format_data(sample) for sample in train_dataset]\n",
    "val_dataset = [format_data(sample) for sample in val_dataset]\n",
    "test_dataset = [format_data(sample) for sample in test_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edb64952-9e5c-44db-9d7b-1cc9aad5ba18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU allocated memory: 0.00 GB\n",
      "GPU reserved memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    model = None\n",
    "    processor = None\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a450a1b-9592-40f7-9f09-5ac4050fb305",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 18:22:21.739371: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-30 18:22:22.279055: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-30 18:22:22.465963: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8473] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-30 18:22:22.516201: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1471] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-30 18:22:22.908453: I tensorflow/core/platform/cpu_feature_guard.cc:211] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a51a58611a940eca96d39e8808f572e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n",
    "\n",
    "# BitsAndBytesConfig int-4 config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=bnb_config\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b247da4-5e28-4001-8212-2fe73e9eb29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,523,136 || all params: 8,294,689,792 || trainable%: 0.0304\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=8,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply PEFT model adaptation\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd48cd71-8b22-4a98-8e75-2d8c28c8b01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387cf5b1-566c-4324-bf65-039e38fef858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "\n",
    "# Configure training arguments\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"output_dir\",  # Directory to save the model\n",
    "    num_train_epochs=8,  # Number of training epochs\n",
    "    per_device_train_batch_size=4,  # Batch size for training\n",
    "    per_device_eval_batch_size=4,  # Batch size for evaluation\n",
    "    gradient_accumulation_steps=8,  # Steps to accumulate gradients\n",
    "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
    "    # Optimizer and scheduler settings\n",
    "    optim=\"adamw_torch_fused\",  # Optimizer type\n",
    "    learning_rate=2e-4,  # Learning rate for training\n",
    "    lr_scheduler_type=\"constant\",  # Type of learning rate scheduler\n",
    "    # Logging and evaluation\n",
    "    logging_steps=10,  # Steps interval for logging\n",
    "    eval_steps=100,  # Steps interval for evaluation\n",
    "    eval_strategy=\"steps\",  # Strategy for evaluation\n",
    "    save_strategy=\"steps\",  # Strategy for saving the model\n",
    "    save_steps=100,  # Steps interval for saving\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to evaluate the best model\n",
    "    greater_is_better=False,  # Whether higher metric values are better\n",
    "    load_best_model_at_end=True,  # Load the best model after training\n",
    "    # Mixed precision and gradient settings\n",
    "    bf16=True,  # Use bfloat16 precision\n",
    "    tf32=True,  # Use TensorFloat-32 precision\n",
    "    max_grad_norm=0.3,  # Maximum norm for gradient clipping\n",
    "    warmup_ratio=0.03,  # Ratio of total steps for warmup\n",
    "    # Hub and reporting\n",
    "    push_to_hub=True,  # Whether to push model to Hugging Face Hub\n",
    "    report_to=\"none\",  # Reporting tool for tracking metrics\n",
    "    # Gradient checkpointing settings\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},  # Options for gradient checkpointing\n",
    "    # Dataset configuration\n",
    "    dataset_text_field=\"\",  # Text field in dataset\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},  # Additional dataset options\n",
    "    # max_seq_length=1024  # Maximum sequence length for input\n",
    ")\n",
    "\n",
    "training_args.remove_unused_columns = False  # Keep unused columns in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f80eece-11a7-484f-8dd8-b996584c886f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import Qwen2_5_VLProcessor\n",
    "# Create a data collator to encode text and image pairs\n",
    "def collate_fn(examples):\n",
    "    # Get the texts and images, and apply the chat template\n",
    "    texts = [\n",
    "        processor.apply_chat_template(example, tokenize=False) for example in examples\n",
    "    ]  # Prepare texts for processing\n",
    "    image_inputs = [process_vision_info(example)[0] for example in examples]  # Process the images to extract inputs\n",
    "\n",
    "    # Tokenize the texts and process the images\n",
    "    batch = processor(\n",
    "        text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n",
    "    )  # Encode texts and images into tensors\n",
    "\n",
    "    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n",
    "    labels = batch[\"input_ids\"].clone()  # Clone input IDs for labels\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels\n",
    "\n",
    "    # Ignore the image token index in the loss computation (model specific)\n",
    "    if isinstance(processor, Qwen2_5_VLProcessor):  # Check if the processor is Qwen2VLProcessor\n",
    "        image_tokens = [151652, 151653, 151655]  # Specific image token IDs for Qwen2VLProcessor\n",
    "    else:\n",
    "        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]  # Convert image token to ID\n",
    "\n",
    "    # Mask image token IDs in the labels\n",
    "    for image_token_id in image_tokens:\n",
    "        labels[labels == image_token_id] = -100  # Mask image token IDs in labels\n",
    "\n",
    "    batch[\"labels\"] = labels  # Add labels to the batch\n",
    "\n",
    "    return batch  # Return the prepared batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbd2f084-6991-4e1b-81e5-cfad965669b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "/storage/plzen4-ntis/home/afremund/.local-tensorflow24.12-r4.simg/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc18e60-cee0-46b0-a2a7-fccf9a357dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='731' max='2152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 731/2152 1:40:06 < 3:15:08, 0.12 it/s, Epoch 2.70/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.020307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>0.016993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.015900</td>\n",
       "      <td>0.015199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.014078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.013737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.013229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.012777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6a80c0-6598-4c3b-ba94-e3de36baa2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(training_args.output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
